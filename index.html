<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="“ <link rel="author" href="https://matrix-alpha.github.io/">

<!-- Fonts and stuff -->
<link href="./files/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./files/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./files/iconize.css">
<script async="" src="./files/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">

      <div class="section head">
        <h1>Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation</h1>

	<div class="authors">
	  <a href="https://matrix-alpha.github.io/">Zhiwei Zhang</a><sup>134</sup>*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://matrix-alpha.github.io/">Yuliang Liu</a><sup>123</sup>*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <!-- <a href="http://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a><sup>12✉</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
	  <!-- <a href="http://dahua.site/">Dahua Lin</a><sup>12✉</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
	</div>

	<div class="affiliations">
	  <a href=""><sup>1</sup>The Chinese University of Hong Kong</a><br>
	  <a href=""><sup>2</sup>Huazhong University of Science and Technology</a><br>
	  <a href=""><sup>3</sup>Centre for Perceptual and Interactive Intelligence</a><br>
	  <a href=""><sup>4</sup>HyperEvol Lab</a><br>
	</div>
    </div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-justified">
          <p>
	      The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, the academia community lacks a dataset that can validate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we construct two new multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K), both featuring visual and text-based inputs and outputs. Additionally, to enable the multimodal system to reject human requests (i.e., demonstrate accountability), as in language-based ChatGPT conversations, we develop and incorporate specific rules into the datasets as supervisory signals. This allows the trained VLM to provide a yes or no answer after visual and textual reasoning, accompanied by a language explanation as to why the human instruction cannot be excuted. In our method, we propose a two-state training procedure to train the image auto-encoder and auto-regressive transformer from scratch. The first state involves a discrete variational autoencoder (dVAE) to compress each image into short tokens, which are then concatenated with text tokens as a single data stream to be fed into the decoder-based transformer for generating visual re-creation and textual feedback in the second state. We provide comprehensive analyses of experimental results in terms of re-created image quality, answer accuracy, and the model behavior when faced with uncertainty and imperfect user queries. We hope our explorations and findings contribute valuable insights regarding the accountability of textual-visual generative models.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
					       
<section class="section">				  
  <div class="container is-max-desktop">
    <!--/ Paper arch. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Framework</h2>
        <!-- <img src='./atvc/atvc.png'></img> -->
        <div class="content has-text-justified">
          <!-- <p>The left figure shows that a multimodal generative model can simultaneously recreate visual input and give textual feedback when faced with human instructions, especially it can reject some commands. The right figure illustrates the overall framework of our method. The model is required to generate a re-created image (M) and a textual feedback (A) conditioned on the visual input (V) and text-based user query (T), and the language-based explanation is also given for those instructions that cannot be executed and the prohibited instructions. </p> -->
        </div>
      </div>
    </div>
</section>
					       
<center><img src="./atvc/atvc.png" border="0" width="80%"></center>
<section class="section">				  
  <div class="container is-max-desktop">
    <!--/ Paper arch. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>The left figure shows that a multimodal generative model can simultaneously recreate visual input and give textual feedback when faced with human instructions, especially it can reject some commands. The right figure illustrates the overall framework of our method. The model is required to generate a re-created image (M) and a textual feedback (A) conditioned on the visual input (V) and text-based user query (T), and the language-based explanation is also given for those instructions that cannot be executed and the prohibited instructions. </p>
        </div>
      </div>
    </div>
</section>

					       
<section class="section">				  
  <div class="container is-max-desktop">
    <!--/ Paper arch. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Experiments</h2>
        <!-- <img src='./atvc/atvc.png'></img> -->
        <div class="content has-text-justified">
          <p> </p>
        </div>
      </div>
    </div>
</section>
					       
<center><img src="./atvc/CLEVR-ATVC.png" border="0" width="80%"></center>
		
<div class="section data">
	<h2>Datasets</h2>
</div>

<center><img src="./atvc/info-clevr-atvc.png" border="0" width="80%"></center>
<center><img src="./atvc/info-fruit-atvc.png" border="0" width="80%"></center>
							       
<div class="section data">
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      	<img src="./files/python_api.jpeg" width="30%">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://drive.google.com/drive/folders/1upbW2Dyg5jz31I3ZeQHp6cjnZTySLhn-?usp=drive_link" target="_blank" class="imageLink"><img src="./files/download.jpeg" width="30%"></a>
</div>


<div class="section code">
	<h2>Code and Model</h2>
</div>
			 

<div class="griditem">
      <a href="https://github.com/matrix-alpha/Accountable-Textual-Visual-Chat" target="_blank" class="imageLink"><img src="./files/code.png" width="30%"</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://huggingface.co/zwzhang/Accountable-Textual-Visual-Chat" target="_blank" class="imageLink"><img src="./files/hf-logo.png" width="30%"</a>
</div>

	
<br>
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2023accountable,
  	title={Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation},
  	author={Zhang, Zhiwei and Liu, Yuliang},
  	journal={arXiv preprint arXiv:2303.05983},
  	year={2023}
	}
    </code></pre>
  </div>
</section>

<br>
	    
	    


</body></html>
