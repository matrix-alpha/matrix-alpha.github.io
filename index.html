<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="The release of ChatGPT and GPT-4 has brought widespread attention to the multimodal dialogue systems. However, academia community lacks a dataset that can validate the multimodal generation capability of Visual Language Model (VLM) on textual-visual chat task. In this paper, we first construct two new multimodal datasets, synthetic CLEVR-ATVC dataset (620K) and manually pictured Fruit-ATVC dataset (50K), that boasts visual and text-based inputs and outputs. In the resulting text-based visual re-creation task, one text-image pair as the query is fed into the machine, and the VLM needs to output both re-created image and textual feedback. In order for the multimodal system to be able to reject human requests (i.e., be accountable), as in language-based ChatGPT conversations, we develop and add some rules to the datasets as supervision signals, so that the trained VLM can give a yes or no answer after visual and textual reasoning and a language explanation is given why the human instruction cannot be executed. In our method, we propose a two-state training procedure to train the image auto-encoder and auto-regressive transformer from scratch. The former one is a discrete variational autoencoder (dVAE) to compress each image into short tokens, then the text tokens are concatenated together as a single stream of data to be fed into the latter one. The decoder-based transformer is used to generate the visual re-creation and textual feedback. We provide comprehensive analysis of experimental results in re-created image quality, answer accuracy, and the model behavior in the face of uncertainty and imperfect user queries. We hope our explorations and findings can bring valuable insights about the accountability of textual-visual generative models." <meta name="keywords" content="computer vision; deep learning">
<link rel="author" href="https://matrix-alpha.github.io/">

<!-- Fonts and stuff -->
<link href="./files/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./files/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./files/iconize.css">
<script async="" src="./files/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">

      <div class="section head">
        <h1>Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation</h1>

	<div class="authors">
	  <a href="https://matrix-alpha.github.io/">Zhiwei Zhang</a><sup>134</sup>*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://scholar.google.com.hk/citations?user=W4htBesAAAAJ&hl=en">Yuliang Liu</a><sup>123</sup>*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <!-- <a href="http://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a><sup>12✉</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
	  <!-- <a href="http://dahua.site/">Dahua Lin</a><sup>12✉</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
	</div>

	<div class="affiliations">
	  <a href="http://mmlab.ie.cuhk.edu.hk/"><sup>1</sup>The Chinese University of Hong Kong</a><br>
	  <a href="http://english.hust.edu.cn/"><sup>2</sup>Huazhong University of Science and Technology</a><br>
	  <a href=""><sup>3</sup>Centre for Perceptual and Interactive Intelligence</a><br>
		<a href=""><sup>4</sup>HyperEvol Research</a><br>
	</div>
    </div>

    <center><img src="./atvc/atvc.png" border="0" width="80%"></center>
	    
    <center><img src="./atvc/CLEVR-ATVC.png" border="0" width="80%"></center>

	    
<div class="section code">
	<h2>Code and Model</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="" target="_blank" class="imageLink"><img src="./files/code.png"></a><br>
		  <!--<a href="" target="_blank">Code and Model</a>-->
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>

<br>

<div class="section data">
	<h2>Datasets</h2>
	<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      	<img src="./files/python_api.jpeg" width="30%">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://drive.google.com/drive/folders/1upbW2Dyg5jz31I3ZeQHp6cjnZTySLhn-?usp=drive_link" target="_blank" class="imageLink"><img src="./files/download.jpeg" width="30%"></a>
    </div>

<br>
	    
	    


</body></html>
