<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="“ <link rel="author" href="https://matrix-alpha.github.io/">

<!-- Fonts and stuff -->
<link href="./files/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./files/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./files/iconize.css">
<script async="" src="./files/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">

      <div class="section head">
        <h1>Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation</h1>

	<div class="authors">
	  <a href="https://matrix-alpha.github.io/">Zhiwei Zhang</a><sup>134</sup>*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://scholar.google.com.hk/citations?user=W4htBesAAAAJ&hl=en">Yuliang Liu</a><sup>123</sup>*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <!-- <a href="http://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a><sup>12✉</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
	  <!-- <a href="http://dahua.site/">Dahua Lin</a><sup>12✉</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -->
	</div>

	<div class="affiliations">
	  <a href="http://mmlab.ie.cuhk.edu.hk/"><sup>1</sup>The Chinese University of Hong Kong</a><br>
	  <a href="http://english.hust.edu.cn/"><sup>2</sup>Huazhong University of Science and Technology</a><br>
	  <a href=""><sup>3</sup>Centre for Perceptual and Interactive Intelligence</a><br>
		<a href=""><sup>4</sup>HyperEvol Lab</a><br>
	</div>
    </div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-justified">
          <p>
	      The release of ChatGPT and GPT-4 has brought widespread attention to the multimodal dialogue systems. However, academia community lacks a dataset that can validate the multimodal generation capability of Visual Language Model (VLM) on textual-visual chat task. In this paper, we first construct two new multimodal datasets, synthetic CLEVR-ATVC dataset (620K) and manually pictured Fruit-ATVC dataset (50K), that boasts visual and text-based inputs and outputs. In the resulting text-based visual re-creation task, one text-image pair as the query is fed into the machine, and the VLM needs to output both re-created image and textual feedback. In order for the multimodal system to be able to reject human requests (i.e., be accountable), as in language-based ChatGPT conversations, we develop and add some rules to the datasets as supervision signals, so that the trained VLM can give a yes or no answer after visual and textual reasoning and a language explanation is given why the human instruction cannot be executed.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>
					       
<section class="section">				  
  <div class="container is-max-desktop">
    <!--/ Paper arch. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Framework</h2>
        <!-- <img src='./atvc/atvc.png'></img> -->
        <div class="content has-text-justified">
          <p>The left figure shows that a multimodal generative model can simultaneously recreate visual input and give textual feedback when faced with human instructions, especially it can reject some commands. The right figure illustrates the overall framework of our method. The model is required to generate a re-created image (M) and a textual feedback (A) conditioned on the visual input (V) and text-based user query (T), and the language-based explanation is also given for those instructions that cannot be executed and the prohibited instructions. </p>
        </div>
      </div>
    </div>
</section>
					       
    <center><img src="./atvc/atvc.png" border="0" width="80%"></center>


<section class="section">				  
  <div class="container is-max-desktop">
    <!--/ Paper arch. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Experiments</h2>
        <!-- <img src='./atvc/atvc.png'></img> -->
        <div class="content has-text-justified">
          <p> </p>
        </div>
      </div>
    </div>
</section>
					       
<center><img src="./atvc/CLEVR-ATVC.png" border="0" width="80%"></center>
							       
							       
<div class="section code">
	<h2>Code and Model</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="" target="_blank" class="imageLink"><img src="./files/code.png"></a><br>
		  <!--<a href="" target="_blank">Code and Model</a>-->
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>

<br>

<div class="section data">
	<h2>Datasets</h2>
	<br>
	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      	<img src="./files/python_api.jpeg" width="30%">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://drive.google.com/drive/folders/1upbW2Dyg5jz31I3ZeQHp6cjnZTySLhn-?usp=drive_link" target="_blank" class="imageLink"><img src="./files/download.jpeg" width="30%"></a>
</div>
	

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2023accountable,
  	title={Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation},
  	author={Zhang, Zhiwei and Liu, Yuliang},
  	journal={arXiv preprint arXiv:2303.05983},
  	year={2023}
	}
    </code></pre>
  </div>
</section>

<br>
	    
	    


</body></html>
